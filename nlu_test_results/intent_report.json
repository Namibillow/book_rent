{
  "terminate_conversation": {
    "precision": 0.6,
    "recall": 1.0,
    "f1-score": 0.7499999999999999,
    "support": 3,
    "confused_with": {}
  },
  "repeat_again": {
    "precision": 0.6,
    "recall": 1.0,
    "f1-score": 0.7499999999999999,
    "support": 3,
    "confused_with": {}
  },
  "select_from_list": {
    "precision": 1.0,
    "recall": 0.9565217391304348,
    "f1-score": 0.9777777777777777,
    "support": 23,
    "confused_with": {
      "inform_book_info": 1
    }
  },
  "search_book": {
    "precision": 0.78125,
    "recall": 0.9259259259259259,
    "f1-score": 0.847457627118644,
    "support": 27,
    "confused_with": {
      "out_of_scope": 1,
      "inform_book_info": 1
    }
  },
  "help": {
    "precision": 0.8571428571428571,
    "recall": 1.0,
    "f1-score": 0.923076923076923,
    "support": 6,
    "confused_with": {}
  },
  "affirm": {
    "precision": 1.0,
    "recall": 0.45454545454545453,
    "f1-score": 0.625,
    "support": 11,
    "confused_with": {
      "search_book": 2,
      "terminate_conversation": 1
    }
  },
  "faq_check_open": {
    "precision": 0.9473684210526315,
    "recall": 0.9,
    "f1-score": 0.9230769230769231,
    "support": 20,
    "confused_with": {
      "user_inventory_check_return": 1,
      "inform_book_info": 1
    }
  },
  "inform_book_info": {
    "precision": 0.7686274509803922,
    "recall": 0.98,
    "f1-score": 0.8615384615384617,
    "support": 200,
    "confused_with": {
      "out_of_scope": 3,
      "user_inventory_check_current_borrowing": 1
    }
  },
  "out_of_scope": {
    "precision": 0.5357142857142857,
    "recall": 0.625,
    "f1-score": 0.576923076923077,
    "support": 24,
    "confused_with": {
      "inform_book_info": 6,
      "contact_library": 1
    }
  },
  "contact_library": {
    "precision": 0.9795918367346939,
    "recall": 0.9230769230769231,
    "f1-score": 0.9504950495049506,
    "support": 52,
    "confused_with": {
      "out_of_scope": 2,
      "terminate_conversation": 1
    }
  },
  "faq_total_allowance": {
    "precision": 0.9411764705882353,
    "recall": 0.9411764705882353,
    "f1-score": 0.9411764705882353,
    "support": 17,
    "confused_with": {
      "user_inventory_check_remaining": 1
    }
  },
  "user_inventory_check_current_borrowing": {
    "precision": 0.987012987012987,
    "recall": 0.5891472868217055,
    "f1-score": 0.7378640776699029,
    "support": 129,
    "confused_with": {
      "inform_book_info": 44,
      "out_of_scope": 4
    }
  },
  "thanks": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4,
    "confused_with": {}
  },
  "greet": {
    "precision": 1.0,
    "recall": 0.2222222222222222,
    "f1-score": 0.3636363636363636,
    "support": 9,
    "confused_with": {
      "help": 1,
      "goodbye": 1
    }
  },
  "faq_check_location": {
    "precision": 0.8181818181818182,
    "recall": 0.8181818181818182,
    "f1-score": 0.8181818181818182,
    "support": 11,
    "confused_with": {
      "search_book": 1,
      "inform_book_info": 1
    }
  },
  "goodbye": {
    "precision": 0.8,
    "recall": 0.8,
    "f1-score": 0.8000000000000002,
    "support": 5,
    "confused_with": {
      "out_of_scope": 1
    }
  },
  "user_inventory_check_remaining": {
    "precision": 0.8181818181818182,
    "recall": 0.9,
    "f1-score": 0.8571428571428572,
    "support": 10,
    "confused_with": {
      "faq_total_allowance": 1
    }
  },
  "user_inventory_check_return": {
    "precision": 0.9847328244274809,
    "recall": 1.0,
    "f1-score": 0.9923076923076923,
    "support": 129,
    "confused_with": {}
  },
  "bot_challenge": {
    "precision": 0.6666666666666666,
    "recall": 0.5,
    "f1-score": 0.5714285714285715,
    "support": 4,
    "confused_with": {
      "inform_book_info": 2
    }
  },
  "deny": {
    "precision": 0.8,
    "recall": 0.6666666666666666,
    "f1-score": 0.7272727272727272,
    "support": 6,
    "confused_with": {
      "search_book": 1,
      "inform_book_info": 1
    }
  },
  "accuracy": 0.86002886002886,
  "macro avg": {
    "precision": 0.8442823718341934,
    "recall": 0.8101232253579693,
    "f1-score": 0.7997178208622462,
    "support": 693
  },
  "weighted avg": {
    "precision": 0.8835628840572011,
    "recall": 0.86002886002886,
    "f1-score": 0.852829001976641,
    "support": 693
  }
}